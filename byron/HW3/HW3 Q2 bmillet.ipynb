{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Tuning\n",
    "\n",
    "In HW 2 we used grid and random search to find the best hyperparameters for our models. However, these methods are often considered inefficient (as many of you experienced first hand)! In this assingment we will leverage bayesian optimization to perform this task quicker and hopefully more effectively. We will leverage a popular fine tuning library called `optuna` to accomplish this. Thankfully there is a nice blog post on how to do this to help you get started. You can find it here: [optuna tutorial](https://medium.com/@becaye-balde/bayesian-sorcery-for-hyperparameter-optimization-using-optuna-1ee4517e89a). \n",
    "\n",
    "Additionally, you can head over to their website to see some additional examples: [https://optuna.org/#code_examples](https://optuna.org/#code_examples)\n",
    "\n",
    "For this assignment use `optuna` to optimize the hyperparameters of a random forest model to predict the heat capacity dataset from HW 2. Follow the same splitting procedure to ensure that materials aren't mixed between the training and testing sets. Perform the optimization for 10 trials and report the best hyperparameters and the R^2 and MAE on the training and testing sets. How does this compare to your results from HW 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "from CBFV.composition import generate_features\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data:   0%|          | 0/4072 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data: 100%|██████████| 4072/4072 [00:00<00:00, 10269.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeaturizing Compositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Features...: 100%|██████████| 4072/4072 [00:00<00:00, 8116.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating Pandas Objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Input Data: 100%|██████████| 475/475 [00:00<00:00, 16381.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFeaturizing Compositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning Features...: 100%|██████████| 475/475 [00:00<00:00, 7896.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating Pandas Objects...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the program:\n",
    "\n",
    "    # open data file\n",
    "df_full = pd.read_csv(\"data\\cp_data_cleaned.csv\")   \n",
    "\n",
    "    # rename Cp column\n",
    "df_full = df_full.rename(columns={'Cp': 'target'})\n",
    "\n",
    "    # split the data into test and training sets, grouped by formula and obtain relevant indices \n",
    "gss = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)\n",
    "train_index, test_index = next(gss.split(df_full, groups=df_full['formula']))\n",
    "    \n",
    "    # generate the train and test dataframes\n",
    "df_train = df_full.iloc[train_index]\n",
    "df_test = df_full.iloc[test_index]\n",
    "\n",
    "    # Featurize the data from the formulae while keeping the T data by extending the features\n",
    "    # names are such that scaling can be done later\n",
    "X_train_unscaled, y_train, formulae_train, skipped_train = generate_features(df_train, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "X_test_unscaled, y_test, formulae_test, skipped_test = generate_features(df_test, elem_prop='oliynyk', drop_duplicates=False, extend_features=True, sum_feat=True)\n",
    "\n",
    "    # I am not planning on scaling the data, but can put that here if needed\n",
    "    # for now, just takes the unscaled data\n",
    "X_train = X_train_unscaled\n",
    "X_test = X_test_unscaled\n",
    "\n",
    "    # define the range for the hyperparameters we will optimize and calculate the starting values for reference\n",
    "n_estimators_min = 2\n",
    "n_estimators_max = 20\n",
    "max_depth_min = 1\n",
    "max_depth_max = 32\n",
    "    # starting values are integer values between the min and max\n",
    "n_estimators_start = round((n_estimators_min + n_estimators_max) / 2)\n",
    "max_depth_start = round((max_depth_min + max_depth_max) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective(trial=None)\n",
    "#\n",
    "# taken from https://medium.com/@becaye-balde/bayesian-sorcery-for-hyperparameter-optimization-using-optuna-1ee4517e89a\n",
    "# on 3/31/2024\n",
    "#\n",
    "# Modified to include trial=None so I can run the trial with the starting conditions\n",
    "#  \n",
    "# \n",
    "def objective(trial=None):\n",
    "    \"\"\"\n",
    "    objective(trial)\n",
    "    Define a search space for the hyperparameters `n_estimators` and `max_depth`\n",
    "    of a random forest model, then train and evaluate it using cross validation.\n",
    "    If no trial object is passed, it will run with starting values\n",
    "    \"\"\"\n",
    "    if not trial == None:\n",
    "        n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
    "        max_depth = int(trial.suggest_int('max_depth', 1, 32, log=True))\n",
    "    else:\n",
    "        n_estimators = n_estimators_start\n",
    "        max_depth = max_depth_start\n",
    "       \n",
    "    clf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_SEED)\n",
    "    \n",
    "    return cross_val_score(clf, X_train, y_train, n_jobs=-1, cv=3).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 14:09:32,541] A new study created in memory with name: no-name-d89bbf0d-29cc-4926-b6cc-b262e22557f9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-31 14:09:34,487] Trial 0 finished with value: 0.9119932543880372 and parameters: {'n_estimators': 7, 'max_depth': 31}. Best is trial 0 with value: 0.9119932543880372.\n",
      "[I 2024-03-31 14:09:36,528] Trial 1 finished with value: 0.9169474360437159 and parameters: {'n_estimators': 9, 'max_depth': 14}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:40,555] Trial 2 finished with value: 0.914440158107228 and parameters: {'n_estimators': 18, 'max_depth': 20}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:42,668] Trial 3 finished with value: 0.9169474360437159 and parameters: {'n_estimators': 9, 'max_depth': 14}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:46,193] Trial 4 finished with value: 0.9095422079237011 and parameters: {'n_estimators': 20, 'max_depth': 10}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:46,505] Trial 5 finished with value: 0.5378224745532519 and parameters: {'n_estimators': 2, 'max_depth': 1}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:47,340] Trial 6 finished with value: 0.7809763568201503 and parameters: {'n_estimators': 20, 'max_depth': 2}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:47,792] Trial 7 finished with value: 0.5779689631321699 and parameters: {'n_estimators': 13, 'max_depth': 1}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:49,908] Trial 8 finished with value: 0.913862447455795 and parameters: {'n_estimators': 8, 'max_depth': 18}. Best is trial 1 with value: 0.9169474360437159.\n",
      "[I 2024-03-31 14:09:52,043] Trial 9 finished with value: 0.9155179142950239 and parameters: {'n_estimators': 12, 'max_depth': 9}. Best is trial 1 with value: 0.9169474360437159.\n"
     ]
    }
   ],
   "source": [
    "# code taken from https://medium.com/@becaye-balde/bayesian-sorcery-for-hyperparameter-optimization-using-optuna-1ee4517e89a\n",
    "# simple implementation of an optuna study with 10 trials that results in the trial object containing the best one    \n",
    "    # create a study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# get the best trial\n",
    "trial = study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting accuracy: 0.9112494165349379\n",
      "Starting hyperparameters: n_estimators: 11, max_depth = 16\n",
      "Optimized accuracy: 0.9169474360437159\n",
      "Best hyperparameters: {'n_estimators': 9, 'max_depth': 14}\n"
     ]
    }
   ],
   "source": [
    "# Print statements for results of optimization for comparison to starting parameters\n",
    "print(f'Starting accuracy: {objective()}')\n",
    "print(f'Starting hyperparameters: n_estimators: {n_estimators_start}, max_depth = {max_depth_start}')\n",
    "print('Optimized accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor\n",
      "R2: 0.922300940294313\n",
      "MAE: 10.859762026141437\n",
      "RMSE: 21.19079467330077\n"
     ]
    }
   ],
   "source": [
    "# generate the RandomForestRegressor rgr using the best trial's hyperparameters \n",
    "# evaluate the model against the test data set\n",
    "\n",
    "# Train predictive model with best hyperparameters:\n",
    "\n",
    "    # Extract the best hyperparameters\n",
    "best_params = trial.params\n",
    "n_estimators = best_params['n_estimators']\n",
    "max_depth = best_params['max_depth']\n",
    "\n",
    "    # Create a new instance of RandomForestRegressor with the best hyperparameters\n",
    "rfr = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_SEED)\n",
    "\n",
    "    # Fit the regressor to the training data\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "    # Generate prediction data from test dataset\n",
    "y_pred = rfr.predict(X_test)\n",
    "\n",
    "    # Calculate metrics (MAE, R2, RSME)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "    # print results\n",
    "print(\"Random Forest Regressor\")\n",
    "print(\"R2:\", r2)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\",rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was used to train an XG Boost model in HW2 Q2.  \n",
    "XGB hyperparameters were optimized using a randomized search over 100 iterations  \n",
    "  \n",
    "Metrics for that model (XGB model from HW2):  \n",
    "R2: 0.9434960074849109  \n",
    "MAE: 11.148077957876104  \n",
    "RMSE: 18.070864511559254  \n",
    "  \n",
    "This is comparable to the results of the RandomForestRegressor with bayesian hyperparemeter tuning using Optuna with only 10 iterations.  \n",
    "  \n",
    "When hyperparameter tuning was performed on the XGB model, it took several hours to run.  \n",
    "The Optuna optimization ran very fast and gave results that were comparable.  \n",
    "It was also much easier to implement.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatInformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
