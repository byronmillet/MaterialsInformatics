{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Hyperparameter Tuning\n",
    "\n",
    "In HW 2 we used grid and random search to find the best hyperparameters for our models. However, these methods are often considered inefficient (as many of you experienced first hand)! In this assingment we will leverage bayesian optimization to perform this task quicker and hopefully more effectively. We will leverage a popular fine tuning library called `optuna` to accomplish this. Thankfully there is a nice blog post on how to do this to help you get started. You can find it here: [optuna tutorial](https://medium.com/@becaye-balde/bayesian-sorcery-for-hyperparameter-optimization-using-optuna-1ee4517e89a). \n",
    "\n",
    "Additionally, you can head over to their website to see some additional examples: [https://optuna.org/#code_examples](https://optuna.org/#code_examples)\n",
    "\n",
    "For this assignment use `optuna` to optimize the hyperparameters of a random forest model to predict the heat capacity dataset from HW 2. Follow the same splitting procedure to ensure that materials aren't mixed between the training and testing sets. Perform the optimization for 10 trials and report the best hyperparameters and the R^2 and MAE on the training and testing sets. How does this compare to your results from HW 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MatInformatics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
